\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,latexsym}

\begin{document}
\begin{center} Extending the idea to higher dimensions, 10/12/21 \end{center}

Consider the system
\[
d \mathbf{x}_t = \mathbf{f}(\mathbf{x}_t) dt + \mathbf{g} d\mathbf{L}_t^\alpha.
\]
Here
\begin{itemize}
\item $\mathbf{x}_t \in \mathbb{R}^n$ is a stochastic process indexed by the continuous time variable $t$; for each fixed $t$, $\mathbf{x}_t$ is a random variable taking values in $\mathbb{R}^n$
\item $\mathbf{f} : \mathbb{R}^n \to \mathbb{R}^n$ is a smooth vector field
\item $\mathbf{g}$ is a diagonal, constant matrix---we will abuse notation and refer to $\mathbf{g}$ as a vector when convenient
\item $\mathbf{L}_t^\alpha \in \mathbb{R}^n$ is a vector of independent Levy alpha-stable processes
\end{itemize}
The characteristic function of the process is
\[
\psi(\mathbf{s}, t) = E[e^{i \mathbf{s} \cdot \mathbf{x}_t}],
\]
where $E$ denotes expected value.  Assume that $\mathbf{x}_t$ has a time-dependent probability density function $p(\mathbf{x},t)$.  Then
\[
\psi(\mathbf{s}, t) = \widehat{p}(\mathbf{s},t) = \int_{\mathbb{R}^n} e^{i \mathbf{s} \cdot \mathbf{y}} p(\mathbf{y},t) \, d \mathbf{y}.
\]
Note that we can recover the density from the characteristic function via the inverse transform
\[
p(\mathbf{y},t) = (2 \pi)^{-n} \int_{\mathbb{R}^n} e^{-i \mathbf{s} \cdot \mathbf{y}} \psi(\mathbf{s},t) \, d \mathbf{s}.
\]

\paragraph{Discrete Time Forward Propagation.} In the forward propagation problem, we assume that $\mathbf{f}$, $\mathbf{g}$, $\alpha$ and the distribution of the initial condition $\mathbf{x}_0$ are known.  Then the task is to compute the distribution of $\mathbf{x}_k$ for $k \geq 1$.  To compute this distribution, we first discretize the SDE in time:
\[
\mathbf{x}_{k+1} = \mathbf{x}_k + \mathbf{f}(\mathbf{x}_k) h + \mathbf{g} \Delta \mathbf{L}^{\alpha}_{k+1},
\]
Here the increment $\Delta \mathbf{L}_{k+1}^{\alpha}$ is a random vector that is independent of $\mathbf{x}_k$.  The $j$-th component of this increment vector has characteristic function
\[
\psi(s_j) = \exp(- h |s_j|^\alpha).
\]
As all the components of $\Delta \mathbf{L}_{k+1}^{\alpha}$ are independent, its characteristic function is the product of all the $\psi(s_j)$ components:
\[
\psi_{\Delta \mathbf{L}^\alpha}(\mathbf{s}) = \exp\left( -h \sum_{j=1}^n |s_j|^\alpha \right).
\]
Multiplying the increment vector by $\mathbf{g}$ results in the characteristic function
\[
\psi_{\mathbf{g} \Delta \mathbf{L}^\alpha}(\mathbf{s}) = \exp\left( -h \sum_{j=1}^n |g_j s_j|^\alpha \right).
\]
Now consider the random variable $\mathbf{x}_{k+1} \, | \, \mathbf{x}_k = \mathbf{y}$.  As $\mathbf{x}_k$ is independent of the increment vector, we can treat $\mathbf{x}_{k+1}$ as translation of $\mathbf{g} \Delta \mathbf{L}^\alpha_{k+1}$ by the constant vector $\mathbf{y} + \mathbf{f}(\mathbf{y}) h$.  Then by standard properties of characteristic functions (or Fourier transforms), we have
\begin{align}
\psi_{\mathbf{x}_{k+1} \, | \, \mathbf{x}_k = \mathbf{y}}(\mathbf{s}) &= \exp\left( i \mathbf{s} \cdot [\mathbf{y} + \mathbf{f}(\mathbf{y}) h] \right) \exp\left( -h \sum_{j=1}^n |g_j s_j|^\alpha \right) \nonumber \\
\label{eqn:prodcharfun}
 &= \prod_{j=1}^n \exp\left( i s_j [y^j + f^j(\mathbf{y}) h] \right) \exp\left( -h |g_j s_j|^\alpha \right)
\end{align}
This factorization is exact.  The discrete-time propagation of density functions is given by the laws of probability:
\[
p_{k+1}(\mathbf{x}) = \int_{y \in \mathbb{R}^n} p_{k+1 \, | \, k}(\mathbf{x} \, | \,\mathbf{y}) p_k(\mathbf{y}) \, d \mathbf{y},
\]
where $p_{k+1 \, | \, k}(\mathbf{x},\mathbf{y})$ is the probability density function of $\mathbf{x}_{k+1} \, | \, \mathbf{x}_k = \mathbf{y}$.  Taking the Fourier transform (from $\mathbf{x}$ to $\mathbf{s}$) we obtain
\[
\psi_{k+1}(\mathbf{s}) = \int_{\mathbf{y} \in \mathbb{R}^n} \psi_{\mathbf{x}_{k+1} \, | \, \mathbf{x}_k = \mathbf{y}}(\mathbf{s}) p_k(\mathbf{y}) \, d \mathbf{y}.
\]
Note that
\[
p_k(\mathbf{y}) = (2 \pi)^{-n} \int_{\mathbf{u} \in \mathbb{R}^n}  e^{-i \mathbf{u} \cdot \mathbf{y}} \psi_k(\mathbf{u}) \, d \mathbf{u}.
\]
Putting these equations together, and using (\ref{eqn:prodcharfun}), we have
\begin{equation}
\label{eqn:charfunevol1}
\psi_{k+1}(\mathbf{s}) = \exp\left( -h \sum_{j=1}^n |g_j s_j|^\alpha \right) \int_{ \mathbf{u} \in \mathbb{R}^n }  \biggl[ (2 \pi)^{-n} \int_{\mathbf{y} \in \mathbb{R}^n} e^{i (\mathbf{s} - \mathbf{u}) \cdot \mathbf{y}} e^{i \mathbf{s} \cdot \mathbf{f}(\mathbf{y}) h }  \, d \mathbf{y} \biggr] \psi_k(\mathbf{u})  \, d \mathbf{u}
\end{equation}
Clearly, the integral in square brackets becomes singular as $h \to 0$, and
\[
(2 \pi)^{-n} \int_{\mathbf{y} \in \mathbb{R}^n} e^{i (\mathbf{s} - \mathbf{u}) \cdot \mathbf{y}}  \, d \mathbf{y} = \delta( \mathbf{s} - \mathbf{u} ).
\]
To get around this, we expand in a series in $h$:
\[
e^{i \mathbf{s} \cdot \mathbf{f}(\mathbf{y}) h } = 1 + i (\mathbf{s} \cdot \mathbf{f}(\mathbf{y}) ) h - (\mathbf{s} \cdot \mathbf{f}(\mathbf{y}))^2 \frac{h^2}{2} + O(h^3)
\]
Then
\begin{multline*}
(2 \pi)^{-n} \int_{\mathbf{y} \in \mathbb{R}^n} e^{i (\mathbf{s} - \mathbf{u}) \cdot \mathbf{y}}  e^{i \mathbf{s} \cdot \mathbf{f}(\mathbf{y}) h } \, d \mathbf{y} = \delta( \mathbf{s} - \mathbf{u} ) + (2 \pi)^{-n} i \mathbf{s}^T \widehat{\mathbf{f}}(\mathbf{s}-\mathbf{u}) h \\
- (2 \pi)^{-n} \mathbf{s}^T \left[ \widehat{\mathbf{f} \mathbf{f}^T}(\mathbf{s} - \mathbf{u}) \right] \mathbf{s} \frac{h^2}{2} + O(h^3)
\end{multline*}
Using this in (\ref{eqn:charfunevol1}), we have
\begin{multline}
	\label{eqn:charfunevol2}
\psi_{k+1}(\mathbf{s}) = \exp\left( -h \sum_{j=1}^n |g_j s_j|^\alpha \right) \biggl[ \psi_k(\mathbf{s}) + (2 \pi)^{-n} i h \mathbf{s}^T \int_{\mathbf{u} \in \mathbb{R}^n } \widehat{\mathbf{f}}(\mathbf{s}-\mathbf{u}) \psi_k(\mathbf{u}) \, d \mathbf{u}   \\
 - (2 \pi)^{-n} \frac{h^2}{2} \mathbf{s}^T  \left( \int_{\mathbf{u} \in \mathbb{R}^n} \widehat{\mathbf{f} \mathbf{f}^T}(\mathbf{s} - \mathbf{u})  \psi_k(\mathbf{u}) \, d \mathbf{u} \right)  \mathbf{s} \biggr] 
\end{multline}
\paragraph{$\mathcal{L}^2$ Drifts.} Let $L$ be a positive integer. For the drift $\mathbf{f} = [f_1, f_2, \ldots, f_n]$, suppose that each $f_m \in \mathcal{L}^2([-L \pi, L \pi]^n, \mathbb{R})$.  We will represent $f_m$ via Fourier series:
\[
c^\mathbf{j}_m = (2 \pi L)^{-n} \int_{ [-L\pi, L\pi]^n } e^{-i \mathbf{j} \cdot \mathbf{x} / L} f_m( \mathbf{x} ) \, d\mathbf{x},
\] 
from which we can form the approximate drift
\[
\phi_m (\mathbf{x}) = \sum_{\mathbf{j}} c^\mathbf{j}_m e^{i \mathbf{j} \cdot \mathbf{x} / L}
\]
We sum over the set
\[
\{ \mathbf{j} \in \mathbb{Z}^n \, | \, -J \leq j_m \leq J \text{ for all } m \}.
\]
Note that the Fourier transform is
\begin{equation}
\label{eqn:L2drifthat}
\widehat{\phi}_m (\mathbf{s}) = (2 \pi)^n \sum_{\mathbf{j}} c^{\mathbf{j}}_m \delta(\mathbf{s} + \mathbf{j}/L)
\end{equation}
For the approximate drift, we also have 
\[
\phi_{m} (\mathbf{x}) \phi_{m'} (\mathbf{x}) = \sum_{\mathbf{j}, \mathbf{j}'} c^{\mathbf{j}}_m c^{\mathbf{j}'}_{m'} e^{i (\mathbf{j}+ \mathbf{j}') \cdot \mathbf{x}/L }.
\]
Let $\mathbf{k} = \mathbf{j} + \mathbf{j}'$.  Then the double sum becomes
\[
\phi_{m} (\mathbf{x}) \phi_{m'} (\mathbf{x}) = \sum_{\mathbf{k}} \left[ \sum_{\mathbf{j}}  c^{\mathbf{j}}_m c^{\mathbf{k} - \mathbf{j}}_{m'} \right]  e^{i \mathbf{k} \cdot \mathbf{x} / L },
\]
where $\mathbf{k}$ is summed over
\[
\{ \mathbf{k} \in \mathbb{Z}^n \, | \, -2 J \leq k_m \leq 2 J \text{ for all } m \}.
\]
Then the Fourier transform is
\begin{equation}
\label{eqn:L2drift2hat}
\widehat{ \phi_m \phi_{m'} }(\mathbf{s}) = (2 \pi)^n \sum_{\mathbf{k}} \left[ \sum_{\mathbf{j}}  c^{\mathbf{j}}_m c^{\mathbf{k} - \mathbf{j}}_{m'} \right] \delta(\mathbf{s} + \mathbf{k} / L)
\end{equation}
Using (\ref{eqn:L2drifthat}) and (\ref{eqn:L2drift2hat}), we have
\begin{align*}
\widehat{\mathbf{f}} (\mathbf{s}) &\approx (2 \pi)^n \sum_{\mathbf{j}} \mathbf{c}^{\mathbf{j}} \delta(\mathbf{s} + \mathbf{j}/L) \\
\widehat{\mathbf{f} \mathbf{f}^T} (\mathbf{s}) &\approx (2 \pi)^n \sum_{\mathbf{k}} \left[ \sum_{\mathbf{j}}  \mathbf{c}^{\mathbf{j}} \left( \mathbf{c}^{\mathbf{k} - \mathbf{j}} \right)^T \right] \delta(\mathbf{s} + \mathbf{k} / L)
\end{align*}
Using these approximations in (\ref{eqn:charfunevol2}), we have
\begin{multline}
	\label{eqn:charfunevol3}
	\psi_{k+1}(\mathbf{s}) = \exp\left( -h \sum_{j=1}^n |g_j s_j|^\alpha \right) \biggl[ \psi_k(\mathbf{s}) + i h \mathbf{s}^T  \sum_{\mathbf{j}} \mathbf{c}^{\mathbf{j}} \psi_k(\mathbf{s} + \mathbf{j}/L)  \\
	- \frac{h^2}{2} \mathbf{s}^T \left(  \sum_{\mathbf{k}} \left( \mathbf{c} \ast  \mathbf{c}^T \right)_{\mathbf{k}}  \psi_k(\mathbf{s} + \mathbf{k} / L) \right) \mathbf{s}   \biggr] 
\end{multline}
\paragraph{Basic Estimation.} Suppose we seek to use time series observations
\[
\{ \mathbf{y}_0, \mathbf{y}_1, \ldots, \mathbf{y}_N \}
\]
to estimate parameters $\theta$ in $\mathbf{f}$.  The log likelihood function is
\begin{align*}
L(\theta) &= \log p(  \mathbf{y}_0, \mathbf{y}_1, \ldots, \mathbf{y}_N \, | \, \theta ) \\
 &= \log \left\{ p(\mathbf{y}_0 \, | \, \theta) \prod_{j=1}^N p_{j \, | \, j-1}( \mathbf{y}_j \, | \, \mathbf{y}_{j-1}, \theta ) \right\} \\
 &= \log p(\mathbf{y}_0) + \sum_{j=1}^N \log p_{j \, | \, j-1}( \mathbf{y}_j \, | \, \mathbf{y}_{j-1}, \theta ) 
\end{align*}
In the basic estimation framework, we assume the \emph{time step of the data} equals $h$, the time step of the numerical integration scheme.  This assumption enables us to
\begin{itemize}
\item use (\ref{eqn:prodcharfun}) in the above expression and
\item substitute $p_k(\mathbf{y}) = \delta(\mathbf{y} - \mathbf{y}_k)$
\end{itemize}
to obtain
\[
\psi_{k+1}(\mathbf{s}) = \prod_{j=1}^n \exp\left( i s_j [y^j_k + f^j(\mathbf{y}_k) h] \right) \exp\left( -h |g_j s_j|^\alpha \right).
\]


\end{document}

